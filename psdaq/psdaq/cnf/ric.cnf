if not platform: platform = '7'

ld_lib_path = f'LD_LIBRARY_PATH={CONDA_PREFIX}/epics/lib/linux-x86_64:{CONDA_PREFIX}/pcas/lib/linux-x86_64'
#epics_env = f'{ld_lib_path}'
#epics_env = 'EPICS_PVA_ADDR_LIST=172.21.159.255'+' '+ld_lib_path
#epics_env = '"EPICS_PVA_ADDR_LIST=172.21.159.255 127.0.0.1"'+' '+ld_lib_path
epics_env = 'EPICS_PVA_ADDR_LIST=172.21.152.78 EPICS_PVA_AUTO_ADDR_LIST=YES'+' '+ld_lib_path
manta_env = f'EPICS_PVA_ADDR_LIST=172.21.140.33 EPICS_PVA_AUTO_ADDR_LIST=NO {ld_lib_path}'
hsd_env = 'EPICS_PVA_ADDR_LIST=172.21.159.255'+' '+ld_lib_path
im2k0_env = ld_lib_path
im5k4_env = 'EPICS_PVA_ADDR_LIST=ctl-tmo-cam-01 EPICS_PVA_AUTO_ADDR_LIST=YES'+' '+ld_lib_path
im6k4_env = 'EPICS_PVA_ADDR_LIST=172.21.132.35 EPICS_PVA_AUTO_ADDR_LIST=NO'+' '+ld_lib_path
sp1k4_env = 'EPICS_PVA_ADDR_LIST=ctl-tmo-cam-02 EPICS_PVA_AUTO_ADDR_LIST=NO'+' '+ld_lib_path
imNkM_env = 'EPICS_PVA_ADDR_LIST=ctl-tmo-cam-01 EPICS_PVA_AUTO_ADDR_LIST=YES'+' '+ld_lib_path
andor_epics_env = 'EPICS_PVA_ADDR_LIST=172.21.140.55 EPICS_PVA_AUTO_ADDR_LIST=YES'+' '+ld_lib_path

# Works for cu_pvs_hxr and cu_pvs_hxr_1
acr_epics_env = 'EPICS_CA_ADDR_LIST=172.27.3.255:5068 EPICS_CA_AUTO_ADDR_LIST=NO '+ld_lib_path

# cu_pvs_hxr_2
acr_epics_env2 = 'EPICS_CA_ADDR_LIST=172.27.3.82:5068 EPICS_CA_AUTO_ADDR_LIST=NO '+ld_lib_path
# cu_pvs_hxr_3
acr_epics_env3 = 'EPICS_CA_ADDR_LIST=172.27.3.83:5068 EPICS_CA_AUTO_ADDR_LIST=NO '+ld_lib_path
# cu_pvs_hxr_4
acr_epics_env4 = 'EPICS_CA_ADDR_LIST=172.27.3.84:5068 EPICS_CA_AUTO_ADDR_LIST=NO '+ld_lib_path
# cu_pvs_hxr_5
acr_epics_env5 = 'EPICS_CA_ADDR_LIST=172.27.3.85:5068 EPICS_CA_AUTO_ADDR_LIST=NO '+ld_lib_path

collect_host = 'drp-srcf-gpu001'

xtpg  = 0                    # TXI/TST
#xtpg = 2                    # TMO
#xtpg = 3                    # RIX
#xtpg = 0                    # TXI
#xtpg = 10                   # FEE
groups = platform #+ ' 6' #+ ' 7' + ' 6'
hutch, user = ('tst', 'tstopr')
auth = ' --user {:} '.format(user)
#url  = ' --url https://pswww.slac.stanford.edu/ws-auth/lgbk/ '
url  = ' --url https://pswww.slac.stanford.edu/ws-auth/devlgbk/ '
cdb  = 'https://pswww.slac.stanford.edu/ws-auth/configdb/ws'     # production  ConfigDb
#cdb  = 'https://pswww.slac.stanford.edu/ws-auth/devconfigdb/ws'  # development ConfigDb

#
#  drp variables
#
prom_dir = f'/cds/group/psdm/psdatmgr/etc/config/prom/{hutch}' # Prometheus
data_dir = '/u1/data' # f'/cds/data/drpsrcf'
trig_dir = f'/cds/home/c/claus/lclsii/daq/runs/eb/data/srcf'

task_set = 'taskset -c 4-63'
pvaAddr  = 'pva_addr=172.21.152.78' # mon001;  Added to EPICS_PVA_ADDR_LIST for getting deadtime in grafana
scripts  = f'script_path={trig_dir}'
network  = 'ep_provider=sockets,ep_domain=eno1'
epixData   = '/cds/data/drpsrcf/temp/claus/xtc/rixx1003721-r0106-s010-c000.xtc2'
#epixScript = '/cds/home/c/claus/lclsii/daq/runs/eb/data/srcf/test_drp.py'
epixScript = '/cds/home/c/claus/lclsii/daq/runs/eb/data/srcf/epixHrEmu.py'
epixKwargs = f'sim_length=110584,xtcfile={epixData},l1aOffset=1000,drp=python,pythonScript={epixScript}'

std_opts = f'-P {hutch} -C {collect_host} -M {prom_dir}' # -k {network}'
std_opts0 = f'{std_opts} -d /dev/datadev_0 -o {data_dir} -k {pvaAddr} -k {network}'
std_opts1 = f'{std_opts} -d /dev/datadev_1 -o {data_dir} -k {pvaAddr} -k {network}'
gpu_opts0 = f'{std_opts} -d /dev/datagpu_0 -o {data_dir} -k {pvaAddr} -k ep_provider=sockets,ep_domain=enp1s0f0'
gpu_opts1 = f'{std_opts} -d /dev/datagpu_1 -o {data_dir} -k {pvaAddr} -k ep_provider=sockets,ep_domain=enp1s0f0'

teb_cmd  = task_set+' teb '            +std_opts + f' -k {scripts} -k {network}'
meb_cmd  = task_set+' monReqServer '   +std_opts + f' -k {network}'

drp_cmd0 = task_set+' drp '            +std_opts0
drp_cmd1 = task_set+' drp '            +std_opts1
pva_cmd0 = task_set+' drp_pva '        +std_opts0
pva_cmd1 = task_set+' drp_pva '        +std_opts1
udp_cmd0 = task_set+' drp_udpencoder ' +std_opts0
udp_cmd1 = task_set+' drp_udpencoder ' +std_opts1
bld_cmd0 = task_set+' drp_bld '        +std_opts0+' -k interface=eno1 -k pva_addr=172.27.131.255'
bld_cmd1 = task_set+' drp_bld '        +std_opts1+' -k interface=eno1 -k pva_addr=172.27.131.255'
gpu_cmd0 = task_set+' drp_gpu '        +gpu_opts0
gpu_cmd1 = task_set+' drp_gpu '        +gpu_opts1

#  bld format is {name}+{type}+{pvname}
gmd  = 'gmd+scbld+EM1K0:GMD:HPS'
xgmd = 'xgmd+scbld+EM2K0:XGMD:HPS'

elog_cfg = f'/cds/group/pcds/dist/pds/{hutch}/misc/elog_{hutch}.txt'

#ea_cfg = f'/cds/group/pcds/dist/pds/{hutch}/misc/epicsArch.txt'
#ea_cfg = f'/cds/group/pcds/dist/pds/tmo/misc/epicsArch_tmo.txt'
ea_cfg = f'/cds/group/pcds/dist/pds/rix/misc/epicsArch.txt'
ea_cmd0 = task_set+' epicsArch '       +std_opts0+' '+ea_cfg
ea_cmd1 = task_set+' epicsArch '       +std_opts1+' '+ea_cfg

if xtpg == 2:                   # TMO
    xpms = 'DAQ:NEH:XPM:2 DAQ:NEH:XPM:4 DAQ:NEH:XPM:6 DAQ:NEH:XPM:0'
elif xtpg == 3:                 # RIX
    xpms = 'DAQ:NEH:XPM:3 DAQ:NEH:XPM:5 DAQ:NEH:XPM:0'
else:
    xpms = f'DAQ:NEH:XPM:{xtpg}'

xpm_pvhost = '172.21.152.78'    # Temporary

#
#  ami variables
#
heartbeat_period = 1000 # units are ms

ami_workers_per_node = 4
ami_worker_nodes = ["drp-srcf-cmp042"]
ami_num_workers = len(ami_worker_nodes)
ami_manager_node = "drp-srcf-cmp042"

# procmgr FLAGS: <port number> static port number to keep executable
#                              running across multiple start/stop commands.
#
# HOST       UNIQUEID      FLAGS  COMMAND+ARGS
# list of processes to run
#   required fields: id, cmd
#   optional fields: host, port, flags, conda, env, rtprio
#     flags:
#        'x' or 'X'  -> xterm: open small or large xterm for process console
#        's'         -> stop: sends ctrl-c to process
#        'u'         -> uniqueid: use 'id' as detector alias (supported by acq, cam, camedt, evr, and simcam)

procmgr_config = [
 {                         id:'xpmpva' ,     flags:'s',   env:epics_env, cmd:f'xpmpva {xpms}'},
 {                         id:'groupca',     flags:'s',   env:epics_env, cmd:f'groupca DAQ:NEH {xtpg} {groups}'},
 {                         id:'procstat',    flags:'p',                  cmd:f'procstat {CONFIGDIR}/p{platform}.cnf.last'},

# {host: 'drp-srcf-mon001', id:'pvrtmon-tst', flags:'s',   env:epics_env, cmd:f'epics_exporter -H tst -M {prom_dir} -I xpm-{xtpg} -P DAQ:NEH:XPM:{xtpg} -G Us:RxLinkUp RunTime Run NumL0Acc L0AccRate NumL0Inp L0InpRate DeadFrac'},
# {host: 'drp-srcf-mon001', id:'pvrtmon-t4',  flags:'s',   env:epics_env, cmd:f'epics_exporter -H tmo -M {prom_dir} -I xpm-4 -P DAQ:NEH:XPM:4 -G Us:RxLinkUp DeadFrac'
# {host: 'drp-srcf-mon001', id:'pvrtmon-t2',  flags:'s',   env:epics_env, cmd:f'epics_exporter -H tst -M {prom_dir} -I tpf -E ca -P TPF -G IN20:1:RXLNKUP,LTU0:1:RXLNKUP,GUNB:1:RXLNKUP'},

# set the phase2 transition timeout to 20s. this is because the teb
# has a 16s timeout for slow PVA detectors coming through the gateway.
# both can be reduced if/when we get consistent behavior from gateways.
# {host: collect_host,      id:'control',     flags:'spu', env:epics_env, cmd:f'control -P {hutch} -B DAQ:NEH -x {xtpg} -C BEAM {auth} {url} -d {cdb}/configDB -t trigger -S 1 -T 20000 -V {elog_cfg}'},
 {host: collect_host,      id:'control',     flags:'spu', env:epics_env, cmd:f'control -P {hutch} -B DAQ:NEH -x {xtpg} -C BEAM {auth} {url} -d {cdb}/configDB -t trigger2 -S 0 -T 20000 -V {elog_cfg}'},
# {host: collect_host,      id:'control',     flags:'spu', env:epics_env, cmd:f'control -P {hutch} -B DAQ:NEH -x {xtpg} -C BEAM {auth} {url} -d {cdb}/configDB -t trigger2 -S 1 -T 20000 -V {elog_cfg} -r /dev/null'},
 {                         id:'control_gui', flags:'p',                  cmd:f'control_gui -H {collect_host} --uris {cdb} --expert {auth} --loglevel WARNING'},

# {host: 'drp-srcf-cmp030', id:'teb0',        flags:'spux',               cmd:f'{teb_cmd} -k eb_timeout=0'},
 {host: 'drp-srcf-cmp034', id:'teb0',        flags:'spux',               cmd:f'{teb_cmd} -v'},

# {host: 'drp-srcf-cmp034', id:'meb0',        flags:'spu',                cmd:f'{meb_cmd} -t {hutch}_meb0 -d -n 8 -q {ami_workers_per_node}'},
# {host: 'drp-srcf-cmp034', id:'shm0',        flags:'s',                  cmd:f'shmemClient -p {hutch}_meb0 -t -R'},
# {host: 'drp-srcf-cmp003', id:'meb1',        flags:'spu',                cmd:f'{meb_cmd} -t {hutch}_meb1 -d -n 64 -q {ami_workers_per_node}'},
# {host: 'drp-srcf-cmp003', id:'shm1',        flags:'s',                  cmd:f'shmemClient -p {hutch}_meb1 -R'},

# {host: 'drp-srcf-cmp003', id:'timing_0',    flags:'spux', env:epics_env, cmd:f'{drp_cmd1} -l 0x1 -D ts -k pebbleBufCount=1048576'},
# {host: 'drp-srcf-cmp001', id:'timing_0',    flags:'spux', env:epics_env, cmd:f'{drp_cmd1} -l 0x2 -D ts -k pebbleBufCount=1048576'},
# {host: 'drp-srcf-cmp029', id:'timing_0',    flags:'spu',                cmd:f'{drp_cmd1} -l 0x8 -D ts -k pebbleBufCount=1048576'},
# {host: 'drp-srcf-cmp028', id:'timing_1',    flags:'spux', env:epics_env, rtprio:'50', cmd:f'{drp_cmd1} -l 0x1 -D ts -W 1 -k pebbleBufCount=1048576,pebbleBufSize=4096'},
# {host: 'drp-srcf-cmp003', id:'timing_1',    flags:'spux', env:epics_env, rtprio:'50', cmd:f'{drp_cmd0} -l 0x4 -D ts -W 1 -k pebbleBufCount=1048576,pebbleBufSize=4096'},
# {host: 'drp-srcf-cmp035', id:'timing_1',    flags:'spux', env:epics_env, rtprio:'50', cmd:f'{drp_cmd1} -l 0x1 -D ts -k pebbleBufCount=1048576,pebbleBufSize=4096'},
# {host: 'drp-srcf-cmp043', id:'timing_1',    flags:'spux', env:epics_env, cmd:f'{drp_cmd0} -l 0x1 -D ts -W 1 -k pebbleBufCount=1048576,pebbleBufSize=4096'},
# {host: 'drp-srcf-cmp043', id:'timing_3',    flags:'spux', env:epics_env, cmd:f'{drp_cmd0} -v -l 0x8 -D ts -W 1 -k pebbleBufCount=1048576,pebbleBufSize=4096'},
 {host: 'drp-srcf-gpu001', id:'timing_3',    flags:'spux', env:epics_env, cmd:f'{task_set} drp {gpu_opts1} -v -l 0x8 -D ts -W 1 -k pebbleBufCount=1048576,pebbleBufSize=4096'},

# {host: 'drp-srcf-cmp028', id:'bld_0',       flags:'spux', env:epics_env, cmd:f'{bld_cmd1} -l 0x2 -D ebeam,gmd,xgmd,pcav -k pebbleBufCount=1048576'},
# {host: 'drp-srcf-cmp028', id:'bld_0',       flags:'spux', env:epics_env, cmd:f'{bld_cmd1} -l 0x2 -D gmd'},
# {host: 'drp-srcf-cmp003', id:'bld_0',       flags:'spux', env:epics_env, cmd:bld_cmd0+' -l 0x8 -D '+f'{gmd},{xgmd} -v'},

# {host: 'drp-srcf-cmp028', id:'epics_0',     flags:'spu', env:epics_env, cmd:f'{ea_cmd1} -l 0x4'},
# {host: 'drp-srcf-cmp001', id:'epics_0',     flags:'spu', env:epics_env, cmd:f'{ea_cmd1} -l 0x4'},

# { host: 'drp-srcf-cmp002', id:'mono_encoder_0', flags:'spu', rtprio:'50', cmd:udp_cmd0+' -l 0x4 -L 5007'},  # Loopback
# { host: 'drp-srcf-cmp002', id:'mono_encoder_0', flags:'spu', rtprio:'50', cmd:udp_cmd0+' -l 0x4'},
# { host: 'drp-neh-ctl002',  id:'sim_udpencoder', flags:'sp',               cmd:'sim_udpencoder -a 172.21.152.23 -d 5008'},

# {host: 'drp-srcf-cmp004', id:'tmo_fim0_0',  flags:'spu', env:epics_env, cmd:f'{drp_cmd0} -l 0x1 -D wave8 -k epics_prefix=TST:FIM:W8:01'},

# {host: 'drp-srcf-cmp012', id:'tst_atmpiranha_0', flags:'spux', env:epics_env, cmd:f'{drp_cmd0} -l 0x2 -D piranha4'},
# {host: 'drp-srcf-cmp007', id:'tst_atmpiranha_0', flags:'spux', env:epics_env, cmd:drp_cmd0+' -l 0x1 -D piranha4 -k pebbleBufSize=16384'},
# {host: 'drp-srcf-cmp012', id:'tst_atmpiranha_0', flags:'spux', env:epics_env,  cmd:drp_cmd0+' -l 0x2 -D piranha4 -k pebbleBufSize=16384'},

# {host: 'drp-srcf-cmp007', id:'tmo_piranha_0', flags:'spu', env:epics_env, cmd:drp_cmd0+' -l 0x1 -D piranha4'},

# {host: 'drp-srcf-cmp026', id:'tst_atmopal_0', flags:'spux', env:epics_env, cmd:drp_cmd0+' -l 0x1 -D opal -k ttpv=TMO:TIMETOOL:TTALL'},
# {host: 'drp-srcf-cmp026', id:'tt_feedback', flags:'spu', env:epics_env, cmd:'python /cds/group/pcds/dist/pds/tmo/scripts/ttfeedback.py'},
# {host: 'drp-srcf-cmp026', id:'tmo_peppexopal_0', flags:'spu', env:epics_env, cmd:drp_cmd0+' -l 0x4 -D opal'},

# {host: 'drp-srcf-cmp043', id:'tstcam1_0',   flags:'spux',               cmd:f'{drp_cmd0} -v -l 0x2 -D fakecam -k sim_length=145'},
# {host: 'drp-srcf-cmp028', id:'tstcam1_0',   flags:'spux',               cmd:f'{drp_cmd0} -l 0x1 -D fakecam -k sim_length=145 -k pebbleBufCount=1048576'},
# {host: 'drp-srcf-cmp029', id:'tstcam1_0',   flags:'spux',               cmd:f'{drp_cmd0} -l 0x8 -D fakecam -k sim_length=145 -k pebbleBufCount=1048576'},
# {host: 'drp-srcf-cmp035', id:'tstcam1_0',   flags:'spux',               cmd:f'{drp_cmd0} -l 0x1 -D fakecam -k sim_length=145 -k pebbleBufCount=1048576,pebbleBufSize=4096'},
# {host: 'drp-srcf-cmp028', id:'tstcam2_0',   flags:'spux',                cmd:f'{drp_cmd0} -l 0x2 -D fakecam -k sim_length=145'},
# {host: 'drp-srcf-cmp029', id:'tstcam2_0',   flags:'spu',                cmd:f'{drp_cmd0} -l 0x4 -D fakecam -k sim_length=145'},
# {host: 'drp-srcf-cmp036', id:'tstcam2_0',   flags:'spux',               cmd:f'{drp_cmd0} -l 0x1 -D fakecam -k sim_length=145 -k pebbleBufCount=1048576,pebbleBufSize=4096'},
 {host: 'drp-srcf-gpu001', id:'tstcam1_0',   flags:'spux',                cmd:f'{gpu_cmd1} -l 0x1 -D fakecam -k sim_length=145 -vvv'},
# {host: 'drp-srcf-gpu001', id:'tstcam1_0',   flags:'spux',                cmd:f'{task_set} drp {gpu_opts1} -l 0x1 -D fakecam -vvv'},

# {host: 'drp-srcf-cmp029', id:'manta_0',     flags:'spu', env:manta_env, cmd:f'{pva_cmd0} -l 0x4 SL1K2:EXIT:CAM:DATA1:Pva:Image -k pebbleBufSize=8400000'},

# {host: 'drp-srcf-cmp011', id:'tmo_fzpopal_0', flags:'spux', env:epics_env, cmd:drp_cmd0+' -l 0x1 -D opal'},

# { host: 'drp-srcf-cmp026', id:'tst_atmopal_0', flags:'spux', env:epics_env, cmd:drp_cmd0+' -l 0x1 -D opal -k ttpv=TMO:TIMETOOL:TTALL'},
# { host: 'drp-srcf-cmp026', id:'tt_feedback', flags:'spu', env:epics_env, cmd:'python /cds/group/pcds/dist/pds/tmo/scripts/ttfeedback.py' },
# { host: 'drp-srcf-cmp026', id:'tst_peppexopal_0', flags:'spux', env:epics_env, cmd:drp_cmd0+' -l 0x4 -D opal'},

# {host: 'drp-srcf-cmp001', id:'im2k0_0',     flags:'spux', env:im2k0_env, cmd:pva_cmd0+' -l 0x4 IM2K0:XTES:CAM:DATA1:Pva:Image -k pebbleBufSize=8400000'},
# {host: 'drp-srcf-cmp020', id:'im2k0_0',     flags:'spux', env:im2k0_env, cmd:pva_cmd1+' -l 0x8 IM2K0:XTES:CAM:DATA1:Pva:Image -k pebbleBufSize=8400000'},
# {host: 'drp-srcf-cmp001', id:'imNkM_0',     flags:'spu', env:imNkM_env, cmd:pva_cmd1+f' -l 0x8 -k pebbleBufSize={3*8400000} im5k4=IM5K4:PPM:CAM:DATA1:Pva:Image im6k4=IM6K4:PPM:CAM:DATA1:Pva:Image sp1k4=SP1K4:PPM:CAM:DATA1:Pva:Image'},

 #{host: 'drp-srcf-cmp028', id:'im5k4_0',     flags:'spu', env:im5k4_env, cmd:pva_cmd0+' -l 0x2 IM5K4:PPM:CAM:DATA1:Pva:Image -k pebbleBufSize=8400000'},
 #{host: 'drp-srcf-cmp028', id:'im6k4_0',     flags:'spu', env:im6k4_env, cmd:pva_cmd0+' -l 0x4 IM6K4:PPM:CAM:DATA1:Pva:Image -k pebbleBufSize=8400000'},
 #{host: 'drp-srcf-cmp028', id:'sp1k4_0',     flags:'spu', env:sp1k4_env, cmd:pva_cmd0+' -l 0x8 SP1K4:PPM:CAM:DATA1:Pva:Image -k pebbleBufSize=8400000 -k pva_addr='+xpm_pvhost},

# {host: 'drp-srcf-cmp029', id:'andor_dir_0', flags:'spux', env:andor_epics_env, cmd:pva_cmd1+' -l 0x4 RIX:DIR:CAM:01:IMAGE1:Pva:Image -k pebbleBufSize=2098216'},

# {host: 'drp-srcf-cmp029', id:'andor_norm_0', flags:'spux', env:andor_epics_env, cmd:pva_cmd0+' -l 0x2 RIX:NORM:CAM:01:IMAGE1:Pva:Image -k pebbleBufSize=2098216'},

 #{host: 'drp-srcf-cmp029', id:'andor_vls_0', flags:'spux', env:andor_epics_env, cmd:pva_cmd0+' -l 0x2 -k pebbleBufSize=4196432 RIX:VLS:CAM:01:IMAGE1:Pva:Image RIX:DIR:CAM:01:IMAGE1:Pva:Image'},
 #{host: 'drp-srcf-cmp029', id:'andor_vls_0', flags:'spux', env:andor_epics_env, cmd:pva_cmd0+' -l 0x2 RIX:VLS:CAM:01:IMAGE1:Pva:Image -k pebbleBufSize=2098216'},

# {host: 'drp-srcf-cmp029', id:'andor_vls_0', flags:'spu', env:andor_epics_env, cmd:pva_cmd0+' -l 0x1 RIX:VLS:CAM:01:IMAGE1:Pva:Image -k pebbleBufSize=2098216'},

 #{host: 'drp-srcf-cmp029',  id:'andor_0', flags:'spux', env:andor_epics_env, cmd:pva_cmd0+' -l 0x2 -k pebbleBufSize=4196432 -f /cds/home/c/claus/lclsii/daq/runs/eb/data/neh/PvaDetector_example.txt'},

# {host: 'drp-srcf-cmp043',  id:'pvs_0',  flags:'spux', env:acr_epics_env,  cmd:pva_cmd0+' -l 0x2 -f /cds/home/c/claus/lclsii/daq/runs/eb/data/srcf/cu_pvs_hxr.txt'}, # HXR PVs
# {host: 'drp-srcf-cmp043',  id:'pvs_1',  flags:'spux', env:acr_epics_env,  cmd:pva_cmd0+' -l 0x4 -f /cds/home/c/claus/lclsii/daq/runs/eb/data/srcf/cu_pvs_hxr_1.txt'},
# {host: 'drp-srcf-cmp043',  id:'pvs_2',  flags:'spux', env:acr_epics_env2, cmd:pva_cmd1+' -l 0x1 -f /cds/home/c/claus/lclsii/daq/runs/eb/data/srcf/cu_pvs_hxr_2.txt'},
# {host: 'drp-srcf-cmp043',  id:'pvs_3',  flags:'spux', env:acr_epics_env3, cmd:pva_cmd1+' -l 0x2 -f /cds/home/c/claus/lclsii/daq/runs/eb/data/srcf/cu_pvs_hxr_3.txt'},
# {host: 'drp-srcf-cmp043',  id:'pvs_4',  flags:'spux', env:acr_epics_env4, cmd:pva_cmd1+' -l 0x4 -f /cds/home/c/claus/lclsii/daq/runs/eb/data/srcf/cu_pvs_hxr_4.txt'},
# {host: 'drp-srcf-cmp043',  id:'pvs_5',  flags:'spux', env:acr_epics_env5, cmd:pva_cmd1+' -l 0x8 -f /cds/home/c/claus/lclsii/daq/runs/eb/data/srcf/cu_pvs_hxr_5.txt'},

# {host: 'drp-srcf-cmp043',  id:'pvs_1',  flags:'spux', env:acr_epics_env,  cmd:pva_cmd0+' -l 0x4 -f /cds/home/d/dorlhiac/daq_cnfs/txi_pvlist1_cu_hxr.txt'},
# {host: 'drp-srcf-cmp043',  id:'pvs_1',  flags:'spux', env:acr_epics_env,  cmd:pva_cmd0+' -v -l 0x4 -f /cds/home/c/claus/lclsii/daq/runs/eb/data/srcf/txi_pvlist1_cu_hxr.txt'},

# {host: 'drp-srcf-cmp036', id:'gmdstr0_0',  flags:'spux', env:epics_env, cmd:pva_cmd0+' -l 0x2 ca/EM1K0:GMD:HPS:STR0:STREAM_DOUBLE0'},
# {host: 'drp-srcf-cmp028', id:'xgmdstr0_0',  flags:'spu', env:epics_env, cmd:pva_cmd0+' -l 0x2 ca/EM2K0:XGMD:HPS:STR0:STREAM_SHORT0'}, # Electron 1 waveforms

# Run ~/lclsii/daq/runs/eb/toyP4pSvr.py to serve demo:claus:pv:image
# {host: 'drp-srcf-cmp028', id:'tst_pv_0',    flags:'spux', env:epics_env, cmd:f'{pva_cmd0} -l 0x4 demo:claus:pv:image'},

# {host: 'drp-srcf-cmp035', id:'epixhr_emu_0',  flags:'spux',                cmd:f'{drp_cmd0} -l 0x1 -D epixhremu -k {epixKwargs}'},
# {host: 'drp-srcf-cmp035', id:'epixhr_emu_1',  flags:'spux',                cmd:f'{drp_cmd0} -l 0x2 -D epixhremu -k {epixKwargs}'},
# {host: 'drp-srcf-cmp035', id:'epixhr_emu_6',  flags:'spux',                cmd:f'{drp_cmd0} -l 0x4 -D epixhremu -k {epixKwargs}'},
# {host: 'drp-srcf-cmp035', id:'epixhr_emu_7',  flags:'spux',                cmd:f'{drp_cmd0} -l 0x8 -D epixhremu -k {epixKwargs}'},
]

# HSD: 6.4 GS/s digitizer (DEV07_1B, 1A)
if xtpg == 2:
    hsd_epics = 'DAQ:TMO:HSD:1'

    procmgr_hsd = [
        # {host:'drp-srcf-cmp017', id:'hsd_2',  flags:'spu', env:hsd_env, cmd:f'{drp_cmd1} -D hsd -k hsd_epics_prefix={hsd_epics}_1A:A'},
        # {host:'drp-srcf-cmp017', id:'hsd_3',  flags:'spu', env:hsd_env, cmd:f'{drp_cmd0} -D hsd -k hsd_epics_prefix={hsd_epics}_1A:B'},
        {host:'drp-srcf-cmp018', id:'hsd_5',  flags:'spu', env:hsd_env, cmd:f'{drp_cmd0} -D hsd -k hsd_epics_prefix={hsd_epics}_3D:B'},
        {host:'drp-srcf-cmp018', id:'hsd_7',  flags:'spu', env:hsd_env, cmd:f'{drp_cmd1} -D hsd -k hsd_epics_prefix={hsd_epics}_3E:B'},
        {host:'drp-srcf-cmp024', id:'hsd_10', flags:'spu', env:hsd_env, cmd:f'{drp_cmd0} -D hsd -k hsd_epics_prefix={hsd_epics}_DA:B'},
        {host:'drp-srcf-cmp024', id:'hsd_11', flags:'spu', env:hsd_env, cmd:f'{drp_cmd1} -D hsd -k hsd_epics_prefix={hsd_epics}_DA:A'},
    ]
elif xtpg == 3:
    hsd_epics = 'DAQ:RIX:HSD:1'

    procmgr_hsd = [
        {host:'drp-srcf-cmp009', id:'hsd_0',  flags:'spu', env:epics_env, cmd:drp_cmd1+f' -D hsd -k hsd_epics_prefix={hsd_epics}_1A:A'},
        {host:'drp-srcf-cmp009', id:'hsd_1',  flags:'spu', env:epics_env, cmd:drp_cmd0+f' -D hsd -k hsd_epics_prefix={hsd_epics}_1A:B'},
        {host:'drp-srcf-cmp005', id:'hsd_2',  flags:'spu', env:epics_env, cmd:drp_cmd0+f' -D hsd -k hsd_epics_prefix={hsd_epics}_1B:A'},
        {host:'drp-srcf-cmp005', id:'hsd_3',  flags:'spu', env:epics_env, cmd:drp_cmd1+f' -D hsd -k hsd_epics_prefix={hsd_epics}_1B:B'},
    ]
else:
    procmgr_hsd = [
    ]

#procmgr_config.extend(procmgr_hsd)

#
# ami
#
procmgr_ami = [
 {host:ami_manager_node, id:'ami-global',  flags:'s', env:epics_env, cmd:f'ami-global --hutch {hutch} --prometheus-dir {prom_dir} -N 0 -n {ami_num_workers}'},
 {host:ami_manager_node, id:'ami-manager', flags:'s', cmd:f'ami-manager --hutch {hutch} --prometheus-dir {prom_dir} -n {ami_num_workers*ami_workers_per_node} -N {ami_num_workers}'},
 {                       id:'ami-client',  flags:'s', cmd:f'ami-client -H {ami_manager_node} --prometheus-dir {prom_dir} --hutch {hutch}'},
]

#
# ami workers
#
for N, worker_node in enumerate(ami_worker_nodes):
    procmgr_ami.append({host:worker_node, id:f'ami-meb{N}', flags:'spu',
                        cmd:f'{meb_cmd} -d -n 64 -q {ami_workers_per_node}'})
    procmgr_ami.append({host:worker_node, id:f'ami-node_{N}', flags:'s', env:epics_env,
                        cmd:f'ami-node --hutch {hutch} --prometheus-dir {prom_dir} -N {N} -n {ami_workers_per_node} -H {ami_manager_node} --log-level warning worker -b {heartbeat_period} psana2://shmem={hutch}'})

#procmgr_config.extend(procmgr_ami)
