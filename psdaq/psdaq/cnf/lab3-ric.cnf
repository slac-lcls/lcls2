if not platform: platform = '3'

ld_lib_path = f'LD_LIBRARY_PATH={CONDA_PREFIX}/epics/lib/linux-x86_64:{CONDA_PREFIX}/pcas/lib/linux-x86_64'
epics_env = 'EPICS_PVA_ADDR_LIST=172.21.151.255'+' '+ld_lib_path

epics_host = 'daq-tst-dev02'
collect_host = 'drp-tst-acc05'

groups = platform # + ' 7' + ' 2'
hutch, user = ('tst', 'tstopr')
auth = '--user {:} '.format(user)
url  = '--url https://pswww.slac.stanford.edu/ws-auth/devlgbk/'
#cdb  = f'https://pswww.slac.stanford.edu/ws-auth/configdb/ws'     # production  ConfigDb
cdb  = f'https://pswww.slac.stanford.edu/ws-auth/devconfigdb/ws'  # development ConfigDb

#
#  drp variables
#
prom_dir = '/cds/group/psdm/psdatmgr/etc/config/prom' # Prometheus
data_dir = '/u1/data'

#network  = 'ep_fabric="172.21.156.0/22",ep_domain=eno1,ep_provider=sockets'
#network  = 'ep_provider=verbs'
#network  = 'ep_fabric="172.21.148.0/22",ep_domain=eno1'  # Works
#network = 'ep_fabric="172.21.148.0",ep_provider=sockets'  # Doesn't work
network  = 'ep_provider=sockets,ep_domain=eno1'
task_set = 'taskset 0xffbfeffbfe '
pvaAddr  = 'pva_addr=172.21.148.150' # dev02

std_opts = f'-P {hutch} -C {collect_host} -M {prom_dir} -k {network}'
std_opts0 = f'{std_opts} -d /dev/datadev_0 -o {data_dir} -k {pvaAddr}'
std_opts1 = f'{std_opts} -d /dev/datadev_1 -o {data_dir} -k {pvaAddr}'

#ea_cfg   = '/cds/home/c/claus/lclsii/daq/runs/eb/epicsArch.txt'
ea_cfg   = '/cds/group/pcds/dist/pds/rix/misc/epicsArch.txt'

teb_cmd  = f'{task_set} teb '+        std_opts   #+' -1 18 -2 19'
meb_cmd  = f'{task_set} monReqServer {std_opts}' #+' -1 16 -2 17'

drp_cmd0 = f'{task_set} drp '     +std_opts0
drp_cmd1 = f'{task_set} drp '     +std_opts1
bld_cmd0 = f'{task_set} drp_bld ' +std_opts0+f' -k interface=eno2'
bld_cmd1 = f'{task_set} drp_bld ' +std_opts1+f' -k interface=eno2'
pva_cmd0 = f'{task_set} drp_pva ' +std_opts0
pva_cmd1 = f'{task_set} drp_pva ' +std_opts1
ea_cmd0  = f'{task_set} epicsArch {std_opts0} {ea_cfg}'
ea_cmd1  = f'{task_set} epicsArch {std_opts1} {ea_cfg}'

#
#  ami variables
#
heartbeat_period = 1000 # units are ms

ami_workers_per_node = 4
ami_worker_nodes = ["drp-tst-acc05"]
ami_num_workers = len(ami_worker_nodes)
ami_manager_node = "drp-tst-acc01"

#
# Wave8
#
fields = [
               'RawBuffers:TrigCnt',
               'RawBuffers:FifoPauseCnt','RawBuffers:FifoPauseThreshold',
               'Integrators:IntFifoPauseCnt' ,'Integrators:IntFifoPauseThreshold',
               'Integrators:ProcFifoPauseCnt','Integrators:ProcFifoPauseThreshold',
               'TriggerEventManager:TriggerEventBuffer[0]:FifoWrCnt',
               'TriggerEventManager:TriggerEventBuffer[0]:PauseThreshold',
]

# procmgr FLAGS: <port number> static port number to keep executable
#                              running across multiple start/stop commands.
#                "X" open xterm
#                "s" send signal to child when stopping
#
# HOST       UNIQUEID      FLAGS  COMMAND+ARGS
# list of processes to run
#   required fields: id, cmd
#   optional fields: host, port, flags, conda, env, rtprio
#     flags:
#        'x' or 'X'  -> xterm: open small or large xterm for process console
#        's'         -> stop: sends ctrl-c to process
#        'u'         -> uniqueid: use 'id' as detector alias (supported by acq, cam, camedt, evr, and simcam)

procmgr_config = [
 {                         id:'xpmpva',      flags:'s',   env:epics_env, cmd:f'xpmpva DAQ:LAB2:XPM:2'},
 {                         id:'groupca',     flags:'s',   env:epics_env, cmd:f'groupca DAQ:LAB2 2 {groups}'},
 {                         id:'procstat',    flags:'p',                  cmd:f'procstat {CONFIGDIR}/p{platform}.cnf.last'},

#{ host: collect_host,     id:'control',     flags:'spu', env:epics_env, cmd:f'control -P {hutch} -B DAQ:LAB2 -x 2 -C BEAM {auth} {url} -r /dev/null -S 1'},
 { host: collect_host,     id:'control',     flags:'spu', env:epics_env, cmd:f'control -P {hutch} -B DAQ:LAB2 -x 2 -C BEAM {auth} {url} -r /dev/null -S 1 -T 20000'},
 {                         id:'control_gui', flags:'p',                  cmd:f'control_gui -H {collect_host} --expert {auth} --loglevel WARNING'},

 # TEB
 #{ host: 'drp-tst-acc01',  id:'teb0',        flags:'spux', cmd:f'{teb_cmd}'},
 { host: 'drp-tst-dev004',  id:'teb1',        flags:'spu', cmd:f'{teb_cmd}'},

 # MEB
# { host: 'drp-tst-acc05',  id:'meb0',        flags:'spux', cmd:f'{meb_cmd} -d -q 4'},
 #{ host: 'drp-tst-acc05',  id:'shm0',        flags:'s',   cmd:f'shmemClient -p tst -R'},
 #{ host: 'drp-tst-acc01',  id:'meb1',        flags:'spu', cmd:f'{meb_cmd} -d -q 4'},
 #{ host: 'drp-tst-acc01',  id:'shm1',        flags:'s',   cmd:f'shmemClient -p tst -R'},

# # PVA
 #{ host: 'drp-tst-dev004', id:'tmoandor_0',  flags:'pu',   cmd:f'pva_cmd1} -l 0x2 DAQ:LAB2:PVCAM'},
 #{ host: 'drp-tst-dev004', id:'tmomanta_0',  flags:'pu',   cmd:f'pva_cmd1} -l 0x2 -k pebbleBufSize=8388672 TST:GIGE:RIX:01:IMAGE2:Pva:Image'},
# { host: 'drp-tst-dev004', id:'tmomanta_0',  flags:'pu',   cmd:f'pva_cmd1} -l 0x2 TST:GIGE:RIX:01:IMAGE1:Pva:Image'},
# { host: 'drp-tst-dev008', id:'gmd_0',       flags:'pu',   cmd:f'pva_cmd1} -l 0x2 ca/EM1K0:GMD:HPS:STR0:STREAM_SHORT1'},
#
# # EpicsArch
 { host: 'drp-tst-acc01',  id:'epics_0',     flags:'pu',   env:epics_env, cmd:f'{ea_cmd1} -l 0x2'},
#
# # BLD - must currently run on dev018 since that's where the bld enet xface is
# { host: 'daq-tst-dev03',  id:'lclsbldsvc',  flags:'s',    env:epics_env, cmd:f'lclsBldServer -i enp1s0f1 -r 4 -e 40'},
# { host: epics_host,       id:'bldcas',      flags:'s',    env:epics_env, cmd:f'bldcas -P DAQ:LAB2 HPSEX'},
 #{ host: 'drp-tst-dev008', id:'bld_0',       flags:'pu',   env:epics_env, cmd:f'{bld_cmd1} -l 0x4 -D ebeam'},

# # TimeTool
# #{ host: 'drp-tst-dev012', id:'tmott_0',     flags:'pu',   cmd:f'{drp_cmd0} -D tt'},
# #{ host: 'drp-tst-dev003', id:'tmoopaltt_0', flags:'pu',   cmd:f'{drp_cmd0} -D opal'},
#
# # Opal (non-timetool)
# #{ host: 'drp-tst-dev003', id:'tmoopal_0',   flags:'pu',   env:epics_env, cmd:f'{drp_cmd0} -D opal'},
#
# # Timing System
# { host: 'drp-tst-acc05', id:'timing_1',     flags:'spux', cmd:f'{drp_cmd1} -l 0x8 -D ts'},
# { host: 'drp-tst-dev004', id:'timing_1',     flags:'spux', cmd:f'{drp_cmd1} -l 0x8 -D ts'},
 { host: 'drp-tst-acc01', id:'timing_1',     flags:'spux', cmd:f'{drp_cmd1} -l 0x1 -D ts'},
#
# # Wave8
# #{ host: 'drp-tst-dev005', id:'wave8pvs',    flags:'s',    cmd:'wave8pvs --l 0'},
# #{ host: 'drp-tst-dev005', id:'tmowave8_0',  flags:'spu',  cmd:f'{drp_cmd0} -D wave8 -k epics_prefix=DAQ:WAVE8'},
# #{                         id:'pvant',       flags:'s',    env:epics_env, cmd:'pvant --bases DAQ:WAVE8:Top --fields '+' '.join(fields)},
#
# # Cams
# { host: 'drp-tst-acc05',  id:'tmocam0_0',   flags:'spux', cmd:f'{drp_cmd1} -l 0x1 -D fakecam -k sim_length=14'},
# { host: 'drp-tst-acc05',  id:'tmocam1_0',   flags:'spux', cmd:f'{drp_cmd1} -l 0x2 -D fakecam -k sim_length=14'},
# { host: 'drp-tst-acc05',  id:'tmocam2_0',   flags:'spux', cmd:f'{drp_cmd1} -l 0x4 -D fakecam -k sim_length=14'},
]

# HSD: 6.4 GS/s digitizer
node = 0
bus  = 1
pv   = 2
drp  = 3
hsds = [
    ##{node:'daq-tst-dev07',bus:'1b',pv:'DAQ:LAB2:HSD:DEV07_1B',drp:'drp-tst-dev008'},
    #{node:'daq-tst-dev07',bus:'1a',pv:'DAQ:LAB2:HSD:DEV07_1A',drp:'drp-tst-dev010'},
    #{node:'daq-tst-dev07',bus:'88',pv:'DAQ:LAB2:HSD:DEV07_88',drp:'drp-tst-dev019'},
    #{node:'daq-tst-dev07',bus:'89',pv:'DAQ:LAB2:HSD:DEV07_89',drp:'drp-tst-dev020'},
    #{node:'daq-tst-dev07',bus:'b1',pv:'DAQ:LAB2:HSD:DEV07_B1',drp:'drp-tst-dev021'},
    #{node:'daq-tst-dev07',bus:'b2',pv:'DAQ:LAB2:HSD:DEV07_B2',drp:'drp-tst-dev022'},
    ##{node:'daq-tst-dev07',bus:'5e',pv:'DAQ:LAB2:HSD:DEV07_5E',drp:'drp-tst-dev024'},
    #{node:'daq-tst-dev06',bus:'3d',pv:'DAQ:LAB2:HSD:DEV06_3D',drp:'drp-tst-dev023'},
#    {node:'daq-tst-dev07',bus:'3e',pv:'DAQ:LAB2:HSD:DEV07_3E',drp:'drp-tst-dev008'},
]

#segm=0
#pvlist = []
#for e in hsds:
#    procmgr_config.append({ host: e[node], id:'hsdioc_%s'%e[bus], flags:'', env:epics_env, cmd:f'hsd134PVs -P {e[pv]} -d /dev/pcie_adc_%s'%e[bus]})
#    procmgr_config.append({ host: e[drp],  id:'tmohsd_%d'%segm,   flags:'pu',              cmd:f'{drp_cmd0} -D hsd -k hsd_epics_prefix={e[pv]}:A'})
#    segm += 1
#    procmgr_config.append({ host: e[drp],  id:'tmohsd_%d'%segm,   flags:'pu',              cmd:f'{drp_cmd1} -D hsd -k hsd_epics_prefix={e[pv]}:B'})
#    segm += 1
#    pvlist.append(e[pv]+':A')
#    pvlist.append(e[pv]+':B')
#
#if pvlist != []:
#    procmgr_config.append({id:'hsdpva', flags:'', env:epics_env, cmd:f'hsdpva '+' '.join(pvlist)})

#
# ami
#
procmgr_ami = [
# { host:ami_manager_node, id:'ami-global',  flags:'s', env:epics_env, cmd:f'ami-global --hutch {hutch} --prometheus-dir {prom_dir} -N 0 -n {ami_num_workers}' },
# { host:ami_manager_node, id:'ami-manager', flags:'s', cmd:f'ami-manager --hutch {hutch} --prometheus-dir {prom_dir} -n {ami_num_workers*ami_workers_per_node} -N {ami_num_workers}' },
# {                        id:'ami-client',  flags:'s', cmd:f'ami-client -H {ami_manager_node} --prometheus-dir {prom_dir} --hutch {hutch}' },
]
#
# ami workers
#

for N, worker_node in enumerate(ami_worker_nodes):
    procmgr_ami.append({ host:worker_node, id:f'ami-meb{N}', flags:'spu',
                         cmd:f'{meb_cmd} -d -q {ami_workers_per_node}' })
    procmgr_ami.append({ host:worker_node, id:f'ami-node_{N}', flags:'s',
                         cmd:f'ami-node --hutch {hutch} --prometheus-dir {prom_dir} -N {N} -n {ami_workers_per_node} -H {ami_manager_node} --log-level warning worker -b {heartbeat_period} psana2://shmem={hutch}' })

#procmgr_config.extend(procmgr_ami)
