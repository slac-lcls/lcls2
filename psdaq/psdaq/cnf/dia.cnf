if not platform: platform = '1'

import os
ld_lib_path = 'LD_LIBRARY_PATH=%s/epics/lib/linux-x86_64:%s/pcas/lib/linux-x86_64'%((os.getenv('CONDA_PREFIX'),)*2)
epics_env = 'EPICS_PVA_ADDR_LIST=172.21.156.255'+' '+ld_lib_path
xtcav_epics_env = 'EPICS_PVA_ADDR_LIST=192.168.0.1 EPICS_PVA_AUTO_ADDR_LIST=NO'+' '+ld_lib_path
andor_epics_env = 'EPICS_PVA_ADDR_LIST=172.21.156.89 EPICS_PVA_AUTO_ADDR_LIST=NO'+' '+ld_lib_path

collect_host = 'drp-neh-ctl001'

groups = platform
hutch, user, password = ('tmo', 'tmoopr', 'pcds')
auth = ' --user {:} --password {:} '.format(user,password)
url  = ' --url https://pswww.slac.stanford.edu/ws-auth/lgbk/ '
cdb  = 'https://pswww.slac.stanford.edu/ws-auth/configdb/ws'

#
#  drp variables
#
prom_dir = '/cds/group/psdm/psdatmgr/etc/config/prom' # Prometheus
data_dir = '/ffb01/data'

task_set = 'taskset 0xffbfeffbfe '
std_opts = '-P '+hutch+' -C '+collect_host+' -M '+prom_dir+' -o '+data_dir+' -d /dev/datadev_%d'

drp_cmd0 = task_set+'drp '      +(std_opts%0)+' -k batching=no '
drp_cmd1 = task_set+'drp '      +(std_opts%1)+' -k batching=no '
pva_cmd0 = task_set+'drp_pva '  +(std_opts%0)+' -k batching=no '
pva_cmd1 = task_set+'drp_pva '  +(std_opts%1)+' -k batching=no '
bld_cmd0 = task_set+'drp_bld '  +(std_opts%0)+' -k batching=no,interface=eno1 '
bld_cmd1 = task_set+'drp_bld '  +(std_opts%1)+' -k batching=no,interface=eno1 '

ea_cfg = '/cds/group/pcds/dist/pds/tmo/misc/epicsArch_dia.txt'
ea_cmd0 = task_set+'epicsArch '+(std_opts%1)+' -k batching=no '+ea_cfg
ea_cmd1 = task_set+'epicsArch '+(std_opts%1)+' -k batching=no '+ea_cfg

#
#  ami variables
#
heartbeat_period = 1000 # units are ms

ami_workers_per_node = 4
ami_worker_nodes = ["drp-neh-cmp013"]
ami_num_workers = len(ami_worker_nodes)
ami_manager_node = "drp-neh-ctl001"

procmgr_config = [
 #{                         id:'groupca',     flags:'s',   env:epics_env, cmd:'groupca DAQ:NEH 0 '+groups},
 { id:'procstat', flags:'p', cmd:'procstat p'+platform+'.cnf.last'},

 { host: 'drp-neh-cmp016', id:'teb0',        flags:'spu', cmd:'teb '+'-P '+hutch+' -C '+collect_host+' -M '+prom_dir},

 { host: collect_host,     id:'control',     flags:'spu', env:epics_env, cmd:f'control -P {hutch} -B DAQ:NEH -x 0 -C BEAM {auth} {url} -d {cdb}/configDB -r /dev/null -t trigger -S 1'},
 {                         id:'control_gui', flags:'p',                  cmd:f'control_gui -H {collect_host} --uris {cdb} --expert {auth} --loglevel DEBUG'},

 { host: 'drp-neh-cmp010', id:'epics_0',     flags:'spu', env:epics_env, cmd:ea_cmd1+' -l 0x4'},
 { host: 'drp-neh-cmp010', id:'xtcav_0',     flags:'spu', env:xtcav_epics_env, cmd:pva_cmd1+' -l 0x1 OTRS:DMPS:695:RAW -k pebbleBufSize=2097300 -k match_tmo_ms=133'},
# { host: 'drp-neh-cmp010', id:'bld_0',       flags:'spu', env:epics_env, cmd:bld_cmd1+' -l 0x2 -D ebeam,gmd,xgmd,pcav'},
 { host: 'drp-neh-cmp010', id:'bld_0',       flags:'spu', env:epics_env, cmd:bld_cmd1+' -l 0x2 -D ebeam,gmd,xgmd'},
# { host: 'drp-neh-cmp010', id:'andor_0',     flags:'spu', env:andor_epics_env, cmd:pva_cmd1+' -l 0x8 TMO:VLS:CAM:01:IMAGE1:Pva:Image -k pebbleBufSize=2097216'},
 { host: 'drp-neh-cmp014', id:'timing_0',    flags:'spu',                cmd:drp_cmd1+' -l 0x1 -D ts'},
 { host: 'drp-neh-cmp014', id:'xcusbr_0',     flags:'spu', env:epics_env, cmd:pva_cmd1+' -l 0x2 ca/BPMS:CLTS:420:XCUSBR -k pebbleBufSize=8192'},
 { host: 'drp-neh-cmp014', id:'tmitcusbr_0',     flags:'spu', env:epics_env, cmd:pva_cmd1+' -l 0x4 ca/BPMS:UNDS:4790:TMITCUSBR -k pebbleBufSize=8192'},
]

procmgr_ami = [
#
# ami
#
 { host:ami_manager_node, id:'ami-global',  flags:'s', cmd:f'ami-global --hutch {hutch} --prometheus-dir {prom_dir} -N 0 -n {ami_num_workers}' },
 { host:ami_manager_node, id:'ami-manager', flags:'s', cmd:f'ami-manager --hutch {hutch} --prometheus-dir {prom_dir} -n {ami_num_workers*ami_workers_per_node} -N {ami_num_workers}' },
 {                        id:'ami-client',  flags:'s', cmd:f'ami-client -H {ami_manager_node} --prometheus-dir {prom_dir}/{hutch}' },
]

#
# ami workers
#
extra_shmem_queues=1
for N, worker_node in enumerate(ami_worker_nodes):
    procmgr_ami.append({ host:worker_node, id:f'meb{N}', flags:'spu',
                         cmd:f'{task_set} monReqServer -P {hutch} -C {collect_host} -M {prom_dir} -d -q {ami_workers_per_node+extra_shmem_queues}' })
    procmgr_ami.append({ host:worker_node, id:f'ami-node_{N}', flags:'s',
                         cmd:f'ami-node --hutch {hutch} --prometheus-dir {prom_dir} -N {N} -n {ami_workers_per_node} -H {ami_manager_node} --log-level warning worker -b {heartbeat_period} psana://shmem={hutch}' })

procmgr_config.extend(procmgr_ami)
